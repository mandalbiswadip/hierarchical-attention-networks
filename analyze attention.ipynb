{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/biswadip/autoreload_jn.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sentiment\\ analysis-attention.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[2]:\n",
    "import os\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "from models.text import CustomTokenizer\n",
    "from models.model import word_list, max_sentences, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CustomTokenizer(word_list = word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_s = 32\n",
    "\n",
    "path = \"negativeReviews/\"\n",
    "neg_reviews = []\n",
    "for f in os.listdir(path):\n",
    "    file = os.path.join(path, f)\n",
    "    with open(file, \"r\") as fl:\n",
    "        neg_reviews.append(fl.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"positiveReviews//\"\n",
    "pos_reviews = []\n",
    "for f in os.listdir(path):\n",
    "    file = os.path.join(path, f)\n",
    "    with open(file, \"r\") as fl:\n",
    "        pos_reviews.append(fl.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape (25000, 2)\n",
      "Class Distribution 1    12500\n",
      "0    12500\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(\n",
    "    {\"text\":neg_reviews, \"sentiment\":0}\n",
    ").append(pd.DataFrame(\n",
    "    {\"text\":pos_reviews, \"sentiment\":1}\n",
    "))\n",
    "\n",
    "print(\"Data Shape {}\".format(data.shape))\n",
    "# data.to_csv(\"tagged_data.csv\")\n",
    "print(\"Class Distribution {}\".format(\n",
    "    data.sentiment.value_counts())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(tokenizer.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()\n",
    "\n",
    "data = data.filter([\"text\",\"sentiment\"])\n",
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "import tensorflow as tf\n",
    "\n",
    "inp = tokenizer.doc_to_sequences(data.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for doc in inp:\n",
    "    inputs.append(\n",
    "        tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            doc, padding=\"post\", value=0, maxlen=maxlen, dtype=None\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((len(inputs),max_sentences,maxlen))\n",
    "\n",
    "for row,x in zip(a, inputs):\n",
    "    row[:len(x)] = x[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "\n",
    "from models.model import get_model, ModelCheckpoint, max_sentences, maxlen\n",
    "from models.model import HierarchicalAttentionLayer, SequenceAttentionLayer, AdditiveAttention\n",
    "from models.data import Sequence_generator\n",
    "# from models.tuner import tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow' from '/home/biswadip/biswadip_temp/repos/bill_parsing/venv3/lib/python3.6/site-packages/tensorflow/__init__.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    \"model/attention-wh_100_sh_400-10-0.87.h5\",\n",
    "    custom_objects={\"HierarchicalAttentionLayer\": HierarchicalAttentionLayer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hirer_layer = model.layers[1]\n",
    "# model.layers\n",
    "\n",
    "sentence_layer = hirer_layer.sentence_layer\n",
    "document_layer = hirer_layer.document_layer\n",
    "doc_attention_layer = document_layer.attention_layer\n",
    "\n",
    "\n",
    "def get_doc_attention_scores(sent):\n",
    "    \"\"\"Get sentences in attention weight order\"\"\"\n",
    "#     sent = data.text.values[1]\n",
    "    print(\"Original Document: \\n\\n {}\".format(sent))\n",
    "    print(\"\\n\"*3)\n",
    "    sent_tokenized_index = tokenizer.doc_to_sequences([sent])\n",
    "    padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                sent_tokenized_index[0], padding=\"post\", value=0, maxlen=maxlen, dtype=None\n",
    "            )\n",
    "    padded_input = [padded_input]\n",
    "\n",
    "    aa = np.zeros((1,max_sentences,maxlen))\n",
    "\n",
    "    for row,x in zip(aa, padded_input):\n",
    "        row[:len(x)] = x[:max_sentences]\n",
    "    print(model.predict(aa))\n",
    "\n",
    "    sent_output = sentence_layer(aa)\n",
    "    doc_lstm_hiden_states = document_layer.lstm(sent_output)\n",
    "    doc_atten_scores = doc_attention_layer.get_attention_scores(\n",
    "        [doc_lstm_hiden_states, doc_lstm_hiden_states]\n",
    "    )\n",
    "\n",
    "    scores = doc_atten_scores[0][0]\n",
    "\n",
    "    sentencess = tokenizer.tokenize_sentence(sent)\n",
    "    print(tf.sort(scores, direction=\"DESCENDING\")[:len(sentencess)])\n",
    "    print(\"\\n\"*3)\n",
    "\n",
    "    print([sentencess[i] for i in tf.argsort(\n",
    "        scores, direction=\"DESCENDING\") if i<len(sentencess) ])\n",
    "    print([len(sentencess[i]) for i in tf.argsort(\n",
    "        scores, direction=\"DESCENDING\") if i<len(sentencess) ])\n",
    "\n",
    "    return [sentencess[i] for i in tf.argsort(\n",
    "            scores, direction=\"DESCENDING\") if i<len(sentencess) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hirer_layer = model.layers[1]\n",
    "# model.layers\n",
    "\n",
    "sentence_layer = hirer_layer.sentence_layer\n",
    "document_layer = hirer_layer.document_layer\n",
    "doc_attention_layer = document_layer.attention_layer\n",
    "\n",
    "\n",
    "def get_sent_attention_scores(sent):\n",
    "#     sent = data.text.values[1]\n",
    "#     sent = \"The Movie sucks!\"\n",
    "    print(\"Original Sentence: \\n\\n {}\".format(sent))\n",
    "    print(\"\\n\"*3)\n",
    "\n",
    "    sent_tokenized_index = tokenizer.texts_to_sequences([sent])\n",
    "\n",
    "    padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                sent_tokenized_index, padding=\"post\", value=0, maxlen=maxlen, dtype=None\n",
    "            )\n",
    "\n",
    "    padded_input = [padded_input]\n",
    "\n",
    "    aa = np.zeros((1,max_sentences,maxlen))\n",
    "\n",
    "    for row,x in zip(aa, padded_input):\n",
    "        row[:len(x)] = x[:max_sentences]\n",
    "    print(model.predict(aa))\n",
    "\n",
    "    sent_output = sentence_layer(aa)\n",
    "\n",
    "    inputs = sentence_layer.embed(aa)\n",
    "    # putting every sentence in a single axis\n",
    "    inputs_mask = inputs._keras_mask\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape = (-1 ,maxlen ,sentence_layer.embedding_len)\n",
    "    )\n",
    "    mask = tf.reshape(\n",
    "        inputs_mask,\n",
    "        shape=(-1, maxlen)\n",
    "    )\n",
    "\n",
    "    lstm_out = sentence_layer.lstm(inputs, mask=mask)\n",
    "\n",
    "    lstm_mask = lstm_out._keras_mask\n",
    "    attention_scores = sentence_layer.attention_layer.get_attention_scores(\n",
    "        [lstm_out,lstm_out], \n",
    "        mask = [lstm_mask, lstm_mask]\n",
    "    )\n",
    "    word_indexes = tokenizer.texts_to_sequences([sent])[0]\n",
    "\n",
    "    for i in range(len(word_indexes)):\n",
    "        \n",
    "        sent_attention_scores = attention_scores[i]\n",
    "\n",
    "        first_set_attention_scores = sent_attention_scores[0]\n",
    "\n",
    "\n",
    "        sorted_indexes = tf.argsort(\n",
    "            first_set_attention_scores, \n",
    "            direction=\"DESCENDING\"\n",
    "        )\n",
    "\n",
    "        ranked_words = [tokenizer.index_word[word_indexes[wx]] \n",
    "                        for wx in sorted_indexes if wx<len(word_indexes)]\n",
    "        print(ranked_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8f3d059bc9bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# i=10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "# i=10\n",
    "i+=1\n",
    "text = data.text.values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r\"<br /><br />\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"The Movie Was awesome\"\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xxx = get_doc_attention_scores(text)\n",
    "most_important_text = xxx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Important sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Or perhaps it's the worst piece of trash ever made and it should be locked away in a vault and dumped in the Arctic Ocean.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_important_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Ranking + Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word attention weights has ranking as well as interaction information. \n",
    "\n",
    "Attention weights has a dimension of ```(t_steps, tsteps)``` ```t_step``` is the number of words in the sentence. Normally, ```(i,j)``` element of the matrix provides how much interaction there is between ```ith and jth``` words irrespective of their distance. In general you can also take any row and the ```t_steps``` sized vector will produce the ranking of words. \n",
    "\n",
    "It is also observed that all the rows almost same **in case sentence attention vectors**. The attention weights in case sentences are more useful for ranking. This is expected as it is very difficult to interact sentences as more or less they provide independent information.\n",
    "\n",
    "**In case of Word attention,** the attention weights vary across the rows(not a lot). This provides a sense that the attention weights not only create ranking but also measures the level of interctions between words to create more useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: \n",
      "\n",
      " Or perhaps it's the worst piece of trash ever made and it should be locked away in a vault and dumped in the Arctic Ocean.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[0.71085775 0.28914222]]\n",
      "['ever', 'made', 'and', 'trash', 'it', 'should', 'be', 'away', 'locked', 'vault', 'and', 'in', 'dumped', 'of', 'piece', 'a', 'in', 'the', 'ocean', 'arctic', 'worst', 'perhaps', 'or', 'unk', 'the']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n",
      "['or', 'perhaps', 'unk', 'the', 'worst', 'piece', 'of', 'trash', 'ever', 'made', 'and', 'it', 'should', 'be', 'locked', 'away', 'in', 'a', 'vault', 'and', 'dumped', 'in', 'the', 'arctic', 'ocean']\n"
     ]
    }
   ],
   "source": [
    "get_sent_attention_scores(most_important_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv3",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
